{"cells":[{"cell_type":"markdown","metadata":{"id":"e1cabd9e"},"source":["# Project 3: Kaggle Competition - Semantic Segmentation\n","\n","\n","#### <font style=\"color:green\">Maximum Points: 100</font>\n","\n","<div>\n","    <table>\n","        <tr><td><h3>Sr. no.</h3></td> <td><h3><center>Sections</center></h3></td> <td><h3><center>Points</center></h3></td> </tr>\n","        <tr><td><h3>1</h3></td> <td><h3>Custom Dataset Class</h3></td> <td><h3><center>7</center></h3></td> </tr>\n","        <tr><td><h3>2</h3></td> <td><h3>Visualize Dataset</h3></td> <td><h3><center>3</center></h3></td> </tr>\n","        <tr><td><h3>3</h3></td> <td><h3>Loss Function</h3></td> <td><h3><center>5</center></h3></td> </tr>\n","        <tr><td><h3>4</h3></td> <td><h3>Evaluation Metrics</h3></td> <td><h3><center>5</center></h3></td> </tr>\n","        <tr><td><h3>5</h3></td> <td><h3>Model</h3></td> <td><h3><center>10</center></h3></td> </tr>\n","        <tr><td><h3>6</h3></td> <td><h3>Train and Plot Results</h3></td> <td><h3><center>7</center></h3></td> </tr>\n","        <tr><td><h3>7</h3></td> <td><h3>Inference</h3></td> <td><h3><center>3</center></h3></td> </tr>\n","        <tr><td><h3>8</h3></td> <td><h3>Prepare Submission CSV</h3></td><td><h3><center>10</center></h3></td> </tr>\n","        <tr><td><h3>9</h3></td> <td><h3>Kaggle Submission Score</h3></td> <td><h3><center>50</center></h3></td> </tr>\n","    </table>\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"7bb69550"},"source":["**Kaggle Submission Score Points Distribution on Public Test Set**\n","\n","\n","<div>\n","    <table>\n","        <tr><td><h3>Sr. no.</h3></td> <td><h3>Public Test Set Dice Score</h3></td> <td><h3><center>Points</center></h3></td> </tr>\n","        <tr><td><h3>1</h3></td> <td><h3> <center>&le; 79%</center></h3></td><td><h3><center>0</center></h3></td> </tr>\n","        <tr><td><h3>2</h3></td> <td><h3><center>80%</center></h3></td><td><h3><center>10</center></h3></td> </tr>\n","        <tr><td><h3>3</h3></td> <td><h3><center>81%</center></h3></td><td><h3><center>20</center></h3></td> </tr>\n","        <tr><td><h3>4</h3></td> <td><h3> <center>82%</center></h3></td><td><h3><center>30</center></h3></td> </tr>\n","        <tr><td><h3>5</h3></td> <td><h3> <center>83%</center></h3></td><td><h3><center>40</center></h3></td> </tr>\n","        <tr><td><h3>6</h3></td> <td><h3><center> &ge; 84%</center></h3></td><td><h3><center>50</center></h3></td> </tr>\n","    </table>\n","</div>\n","\n","**Note: Percentages will be rounded off to the nearest integer.**"]},{"cell_type":"markdown","metadata":{"id":"cc5a67e9"},"source":["**<font style=\"color:red\">Please do not make your notebooks public or publish them on the competition page. You only need to submit your notebook to the lab. This is to make sure that students don't copy each other. You are free to take references from any online resource.</font>**\n","\n","----------"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-12-25 18:01:29.374298: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n","2023-12-25 18:01:29.555627: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-12-25 18:01:29.555677: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-12-25 18:01:29.568175: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-12-25 18:01:29.599777: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n","2023-12-25 18:01:29.600763: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-12-25 18:01:30.723198: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}],"source":["# Colab-gdrive import\n","from google.colab import drive; drive.mount(\"/remote/\")\n","\n","# Built-in imports\n","import os\n","import platform\n","import glob as glob\n","\n","# Third-party imports\n","import numpy as np\n","import cv2 as cv\n","import tensorflow as tf\n","from tensorflow import keras as tfk\n","import albumentations as alb\n","\n","from dataclasses import dataclass\n","# from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n","\n","import matplotlib.pyplot as plt\n","from matplotlib.ticker import (MultipleLocator, FormatStrFormatter)\n","\n","\n","# Custom imports"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[]\n","Using CPU\n"]}],"source":["# System Configuration\n","block_plot = False\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","DISTRIBUTE_STRATEGY = tf.distribute.MirroredStrategy()\n","\n","def system_config(SEED_VALUE):\n","  np.random.seed(SEED_VALUE)\n","  tf.random.set_seed(SEED_VALUE)\n","\n","  # Get list of GPUs.\n","  gpu_devices = tf.config.list_physical_devices('GPU')\n","  print(gpu_devices)\n","\n","  if gpu_devices.__len__() > 0:\n","    print('Using GPU')\n","    os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n","    os.environ['TF_CUDNN_DETERMINISTIC'] = '1' \n","    # If there are any gpu devices, use first gpu.\n","    tf.config.experimental.set_visible_devices(gpu_devices[0], 'GPU')\n","    # Grow the memory usage as it is needed by the process.\n","    # tf.config.experimental.set_memory_growth(gpu_devices[0], True)  ## Throws errrors on colab\n","    # Enable using cudNN.\n","    os.environ['TF_USE_CUDNN'] = \"true\"\n","  else:\n","    print('Using CPU')\n","\n","system_config(7)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'platform' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 34\u001b[0m\n\u001b[1;32m     18\u001b[0m   DATA_TEST_LABELS:  \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/remote/MyDrive/shared/dataset/FloodNet-Supervised-540p_v1.0/test/masks/*.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m   ID2COLOR \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;241m0\u001b[39m: (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m),       \u001b[38;5;66;03m# Background\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;241m1\u001b[39m: (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m),     \u001b[38;5;66;03m# Building Flooded\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;241m9\u001b[39m: (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m),     \u001b[38;5;66;03m# Grass\u001b[39;00m\n\u001b[1;32m     31\u001b[0m   }\n\u001b[1;32m     33\u001b[0m \u001b[38;5;129;43m@dataclass\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfrozen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mTrainingConfig\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m  \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m:\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\n\u001b[1;32m     36\u001b[0m \u001b[43m  \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m:\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\n","Cell \u001b[0;32mIn[4], line 39\u001b[0m, in \u001b[0;36mTrainingConfig\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m EPOCHS:          \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     37\u001b[0m LEARNING_RATE: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[0;32m---> 39\u001b[0m MULTIPROCESSING: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mplatform\u001b[49m\u001b[38;5;241m.\u001b[39msystem()\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLinux\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     41\u001b[0m CHECKPOINT_DIR: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/remote/MyDrive/code/darkest/resources/models/deeplabv3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","\u001b[0;31mNameError\u001b[0m: name 'platform' is not defined"]}],"source":["@dataclass(frozen=True)\n","class DatasetConfig:\n","  \"\"\"Dataset configuration class.\n","\n","  Reference:\n","    https://ieeexplore.ieee.org/document/9460988\n","    https://github.com/BinaLab/FloodNet-Supervised_v1.0\n","\n","  \"\"\"\n","  NUM_CLASSES: int = 10\n","  IMG_WIDTH:   int = 256\n","  IMG_HEIGHT:  int = 256\n","  SHAPE:     tuple = (256, 256, 3)\n","\n","  DATASET_DIR: str = \"/remote/MyDrive/shared/dataset/FloodNet-Supervised-540p_v1.0\"\n","  DATA_TRAIN_IMAGES: str = f\"{DATASET_DIR}/train/images/*.jpg\"\n","  DATA_TRAIN_LABELS: str = f\"{DATASET_DIR}/train/masks/*.png\"\n","  DATA_VALID_IMAGES: str = f\"{DATASET_DIR}/valid/images/*.jpg\"\n","  DATA_VALID_LABELS: str = f\"{DATASET_DIR}/valid/masks/*.png\"\n","  DATA_TEST_IMAGES:  str = f\"{DATASET_DIR}/test/images/*.jpg\"\n","  DATA_TEST_LABELS:  str = f\"{DATASET_DIR}/test/masks/*.png\"\n","\n","  ID2COLOR = {\n","    0: (0, 0, 0),       # Background\n","    1: (255, 0, 0),     # Building Flooded\n","    2: (200, 90, 90),   # Non-Flooded Building\n","    3: (128, 128, 0),   # Road Flooded \n","    4: (155, 155, 155), # Non-Flooded Road\n","    5: (0, 255, 255),   # Water\n","    6: (55, 0, 255),    # Tree\n","    7: (255, 0, 255),   # Vehicle\n","    8: (245, 245, 0),   # Pool\n","    9: (0, 255, 0),     # Grass\n","  }\n","\n","\n","@dataclass(frozen=True)\n","class TrainingConfig:\n","  BATCH_SIZE:      int = 8\n","  EPOCHS:          int = 5\n","  LEARNING_RATE: float = 0.01\n","\n","  MULTIPROCESSING: bool = True if platform.system()==\"Linux\" else False\n","\n","  CHECKPOINT_DIR: str = \"/remote/MyDrive/code/darkest/resources/models/deeplabv3.h5\"\n","\n","\n","@dataclass(frozen=True)\n","class InferenceConfig:\n","  NUM_BATCHES:     int = 3"]},{"cell_type":"markdown","metadata":{"id":"3d0124e5"},"source":["## 1. Custom Dataset Class [7 Points]"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{"id":"1dbd7b2e"},"source":["<font style=\"color:red\">In this section you have to implement your own dataset generator.</font>\n","\n","You can either the `Sequence` class or `tf.data` API or any other method to create training and validation data generators.\n","\n","**Note:** There is no separate validation data, so you will have to create your own validation set by dividing training set into train and validation sets. \n","\n","\n","For example:\n","\n","```python\n","\n","class CustomSegmentationDataset(Sequence):\n","    \n","    def __init__(self, *, batch_size, image_size, image_paths, mask_paths, num_classes, apply_aug):\n","        '''\n","        Generic Dataset class for semantic segmentation datasets\n","        \n","        Arguments:\n","            batch_size:  Number of samples to be included in each batch of data.\n","            image_size:  Image and mask size to be used for training.\n","            image_paths: Path to image directory.\n","            mask_paths:  Path to masks directory.\n","            num_classes: Total number of classes present in dataset.\n","            apply_aug:   Should augmentations be applied.\n","        '''\n","\n","```"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"b1444734"},"outputs":[{"ename":"SyntaxError","evalue":"unterminated triple-quoted string literal (detected at line 101) (774311138.py, line 15)","output_type":"error","traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[2], line 15\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated triple-quoted string literal (detected at line 101)\n"]}],"source":["class SegmentationDataset(tfk.utils.Sequence):\n","  \"\"\"Segmentation dataset class.\n","  \"\"\"\n","\n","  def __init__(self, *, batch_size, image_size, image_paths, mask_paths, num_classes, color_map, apply_aug):\n","    \"\"\"Generic Dataset class for semantic segmentation datasets.\n","\n","    Arguments:\n","      batch_size:  Number of samples to be included in each batch of data.\n","      image_size:  Image and mask size to be used for training.\n","      image_paths: Path to image directory.\n","      mask_paths:  Path to masks directory.\n","      num_classes: Total number of classes present in dataset.\n","      apply_aug:   Should augmentations be applied.\n","\n","    \"\"\"\n","    self.batch_size  = batch_size\n","    self.image_size  = image_size\n","    self.image_paths = image_paths\n","    self.mask_paths  = mask_paths\n","    self.num_classes = num_classes\n","    self.color_map = color_map\n","    self.aug = apply_aug\n","    \n","    self.x = np.empty((self.batch_size,) + self.image_size + (3,), dtype=\"float32\")\n","    self.y = np.empty((self.batch_size,) + self.image_size, dtype=\"float32\")\n","    \n","    if self.aug:\n","      self.train_transforms = self.transforms()\n","    \n","    self.resize_transforms = self.resize()\n","\n","\n","  def __len__(self):\n","    return self.mask_paths.__len__() // self.batch_size\n","\n","\n","  def transforms(self):\n","    train_transforms = alb.Compose([\n","      alb.HorizontalFlip(p=0.5),\n","      alb.ShiftScaleRotate(scale_limit=0.1, rotate_limit=0.2, shift_limit=0.2, p=0.5, border_mode=0)\n","    ])\n","    return train_transforms\n","\n","\n","  def resize(self):\n","    resize_transforms = alb.Resize(\n","      height=self.image_size[0], width=self.image_size[1],\n","      interpolation=cv.INTER_NEAREST,\n","      always_apply=True, p=1\n","    )\n","    return resize_transforms\n","\n","\n","  def reset_array(self):\n","    self.x.fill(0.)\n","    self.y.fill(0.)\n","\n","\n","  def __getitem__(self, idx):\n","    self.reset_array()\n","    i = idx * self.batch_size\n","    batch_image_paths = self.image_paths[i : i + self.batch_size]\n","    batch_mask_paths = self.mask_paths[i : i + self.batch_size]\n","    \n","    for j, (input_image, input_mask) in enumerate(zip(batch_image_paths, batch_mask_paths)):\n","      # Read the image and convert to RGB.\n","      img = cv.imread(input_image)\n","      img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n","\n","      # Read the mask and convert to RGB.\n","      msk = cv.imread(input_mask)\n","      msk = cv.cvtColor(msk, cv.COLOR_BGR2RGB)\n","\n","      # Resize the image and mask.\n","      resized  = self.resize_transforms(image=img, mask=msk)\n","      img, msk = resized['image'], resized['mask']\n","      \n","      if self.aug:\n","        # Apply augmentations.\n","        train_augment = self.train_transforms(image=img, mask=msk)\n","        img, msk = train_augment['image'], train_augment['mask']\n","\n","      # Store image in x.\n","      self.x[j] = img / 255. # Normalizing image to be in range [0.0, 1.0]\n","      \n","      msk = self.rgb_to_onehot(msk, self.color_map, self.num_classes)\n","      \n","      self.y[j] = msk.argmax(-1)\n","      \n","    return self.x, self.y\n","\n","\n","  @classmethod\n","  def rgb_to_onehot(cls, rgb_arr, color_map, num_classes):\n","    shape = rgb_arr.shape[:2] + (num_classes,)\n","    arr = np.zeros( shape, dtype=np.float32 )\n","\n","    for i, classes in enumerate(color_map):\n","      arr[:,:,i] = np.all(rgb_arr.reshape( (-1,3) ) == color_map[i], axis=1).reshape(shape[:2])\n","\n","    return arr"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def create_datasets(aug=False, batch_size:int=TrainingConfig.BATCH_SIZE):\n","  \"\"\"Creates the train and validation datasets from given images - full ds version.\n","\n","  \"\"\"\n","  train_images = sorted(glob.glob(DatasetConfig.DATA_TRAIN_IMAGES))\n","  train_masks  = sorted(glob.glob(DatasetConfig.DATA_TRAIN_LABELS))\n","  valid_images = sorted(glob.glob(DatasetConfig.DATA_VALID_IMAGES))\n","  valid_masks  = sorted(glob.glob(DatasetConfig.DATA_VALID_LABELS))\n","\n","  return (\n","    SegmentationDataset(  # Train data loader.\n","      batch_size=batch_size,\n","      image_size=(DatasetConfig.IMG_HEIGHT, DatasetConfig.IMG_WIDTH),\n","      image_paths=train_images,\n","      mask_paths=train_masks,\n","      num_classes=DatasetConfig.NUM_CLASSES,\n","      color_map=DatasetConfig.ID2COLOR,\n","      apply_aug=aug,\n","    ),\n","    SegmentationDataset(  # Validation data loader.\n","      batch_size=batch_size,\n","      image_size=(DatasetConfig.IMG_HEIGHT, DatasetConfig.IMG_WIDTH),\n","      image_paths=valid_images,\n","      mask_paths=valid_masks,\n","      num_classes=DatasetConfig.NUM_CLASSES,\n","      color_map=DatasetConfig.ID2COLOR,\n","      apply_aug=False,\n","    )\n","  )\n","\n","train_ds, valid_ds = create_datasets(aug=True)"]},{"cell_type":"markdown","metadata":{"id":"c56b0477"},"source":["## 2. Visualize Dataset [3 Points]\n","\n","\n","<font style=\"color:red\">In this section, you have to plot any 3 images from the training set with their corresponding masks and an overlayed image.</font>\n","\n","\n","For example:\n","\n","---\n","---\n","\n","<img src=\"https://learnopencv.com/wp-content/uploads/2022/06/c4-project3-GT.png\">\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to convert a single channel mask representation to an RGB mask.\n","def num_to_rgb(num_arr, color_map=DatasetConfig.ID2COLOR):\n","  single_layer = np.squeeze(num_arr)\n","  output = np.zeros(num_arr.shape[:2]+(3,))\n","  for k in color_map.keys():\n","    output[single_layer==k] = color_map[k]\n","  return np.float32(output) / 255. # return a floating point array in range [0.0, 1.0]\n","\n","\n","def image_overlay(image, segmented_image):\n","  # Function to overlay a segmentation map on top of an RGB image.\n","  alpha = 1.0 # Transparency for the original image.\n","  beta  = 0.7 # Transparency for the segmentation map.\n","  gamma = 0.0 # Scalar added to each sum.\n","  segmented_image = cv.cvtColor(segmented_image, cv.COLOR_RGB2BGR)\n","  image = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n","  image = cv.addWeighted(image, alpha, segmented_image, beta, gamma, image)\n","  image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n","  return np.clip(image, 0.0, 1.0)\n","\n","\n","def display_image_and_mask(data_list, color_mask=True, color_map=DatasetConfig.ID2COLOR):\n","  plt.figure(figsize=(16, 6))\n","  title = ['GT Image', 'GT Mask', 'Overlayed Mask']\n","\n","  grayscale_gt_mask = data_list[1]\n","  \n","  # Create RGB segmentation map from grayscale segmentation map.\n","  rgb_gt_mask = num_to_rgb(data_list[1], color_map=color_map)\n","  \n","  # Create the overlayed image.\n","  overlayed_image = image_overlay(data_list[0], rgb_gt_mask)\n","  \n","  data_list.append(overlayed_image)\n","  \n","  for i in range(len(data_list)):\n","    plt.subplot(1, len(data_list), i+1)\n","    plt.title(title[i])\n","    if title[i] == 'GT Mask':\n","      if color_mask:\n","        plt.imshow(np.array(rgb_gt_mask))\n","      else:\n","        plt.imshow(np.array(grayscale_gt_mask))\n","    else:\n","      plt.imshow(np.array(data_list[i]))\n","    plt.axis('off')\n","\n","  plt.show()"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"799e4478"},"outputs":[{"ename":"NameError","evalue":"name 'valid_ds' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (images, masks) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mvalid_ds\u001b[49m):\n\u001b[1;32m      2\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m: \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m      3\u001b[0m   image, mask \u001b[38;5;241m=\u001b[39m images[\u001b[38;5;241m0\u001b[39m], masks[\u001b[38;5;241m0\u001b[39m]\n","\u001b[0;31mNameError\u001b[0m: name 'valid_ds' is not defined"]}],"source":["for i, (images, masks) in enumerate(train_ds):\n","  if i == 3: break\n","  display_image_and_mask([images[0], masks[0]], color_map=DatasetConfig.ID2COLOR)"]},{"cell_type":"markdown","metadata":{"id":"d230b81d"},"source":["## 3. Loss Function  [5 Points]\n","\n","<font style=\"color:red\">In this section, you have to implement the loss function you will be using for this dataset.</font>\n","\n","1. The loss function can be `Cross-entropy`, `focal loss`, `IoU` or `Dice`.\n","2. You can also use a combination of the above mentioned functions.\n","3. You can also free to implement and use loss functions not taught in this course."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"74a008a6"},"outputs":[],"source":["def focal_loss(y_true, y_pred, gamma=2.0, alpha=0.25):\n","  epsilon = 1e-8\n","  y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n","  focal_loss = - (alpha * tf.pow(1 - y_pred, gamma) * tf.math.log(y_pred))\n","  return tf.reduce_mean(focal_loss)\n","\n","\n","def iou_loss(y_true, y_pred):\n","  intersection = tf.reduce_sum(y_true * y_pred)\n","  union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n","  iou = intersection / union\n","  return 1 - iou\n","\n","\n","def dice_loss(y_true, y_pred):\n","  numerator = 2 * tf.reduce_sum(y_true * y_pred)\n","  denominator = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)\n","  return 1 - (numerator + 1) / (denominator + 1)"]},{"cell_type":"markdown","metadata":{"id":"93928ec0"},"source":["## 4. Evaluation Metrics [5 Points]\n","\n","<font style=\"color:red\">In this section, you have to implement the Dice coefficient evaluation metric.</font>\n","\n","This competition is evaluated on the mean <a href=\"https://en.wikipedia.org/wiki/Sørensen–Dice_coefficient\" target=\"_blank\">Dice coefficient</a>,  which helps compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by:\n","\n","\n","<p>$$DSC =  \\frac{2 |X \\cap Y|}{|X|+ |Y|}$$\n","$$ \\small \\mathrm{where}\\ X = Predicted\\ Set\\ of\\ Pixels,\\ \\ Y = Ground\\ Truth $$ </p>\n","<p>The Dice coefficient is defined to be $1$ when both $X$ and $Y$ are empty.</p>\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"a9bef4ea"},"outputs":[],"source":["def dice_coefficient(y_true, y_pred, smooth=1e-7):\n","  intersection = tf.reduce_sum(y_true * y_pred)\n","  union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)\n","  dice = (2.0 * intersection + smooth) / (union + smooth)\n","  return dice\n","\n","\n","def dice_coefficient_metric(y_true, y_pred):\n","  return dice_coefficient(y_true, y_pred)\n","\n","\n","def mean_iou(y_true, y_pred):\n","  \"\"\"\n","\n","  Arguments:\n","    y_true (ndarray or tensor): Ground truth mask (G). Shape: (batch_size, height, width)\n","      Sparse representation of segmentation mask.\n","\n","    y_pred (ndarray or tensor): Prediction (P) from the model with or without softmax.\n","      Shape: (batch_size, height, width, num_classes).\n","\n","  Returns:\n","    scalar: Classwise mean IoU Metric.\n","\n","  \"\"\"\n","  # Get total number of classes from model output.\n","  num_classes = y_pred.shape[-1]\n","\n","  # Convert single channel (sparse) ground truth labels to one-hot encoding for metric computation.\n","  y_true = tf.one_hot(tf.cast(y_true, tf.int32), num_classes, axis=-1)\n","\n","  # Convert multi-channel predicted output to one-hot encoded thresholded output for metric computation. \n","  y_pred = tf.one_hot(tf.math.argmax(y_pred, axis=-1), num_classes, axis=-1)\n","\n","  # Axes corresponding to image width and height: [B, H, W, C].\n","  axes = (1, 2)\n","\n","  # Intersection: |G ∩ P|. Shape: (batch_size, num_classes)\n","  intersection = tf.math.reduce_sum(y_true * y_pred, axis=axes)\n","\n","  # Total Sum: |G| + |P|. Shape: (batch_size, num_classes)\n","  total = tf.math.reduce_sum(y_true, axis=axes) + tf.math.reduce_sum(y_pred, axis=axes)\n","\n","  # Union: Shape: (batch_size, num_classes)\n","  union = total - intersection\n","\n","  # Boolean (then converted to float) value for each class if it is present or not. \n","  # Shape: (batch_size, num_classes)\n","  is_class_present =  tf.cast(tf.math.not_equal(total, 0), dtype=tf.float32)\n","\n","  # Sum along axis(1) to get number of classes in each image.\n","  # Shape: (batch_size,)\n","  num_classes_present = tf.math.reduce_sum(is_class_present, axis=1)\n","\n","  # Here, we use tf.math.divide_no_nan() to prevent division by 0 (i.e., 0/0 = 0).\n","  # Shape: (batch_size, num_classes)\n","  iou = tf.math.divide_no_nan(intersection, union)\n","\n","  # IoU per image. Average over the total number of classes present in y_true and y_pred.\n","  # Shape: (batch_size,)\n","  iou = tf.math.reduce_sum(iou, axis=1) / num_classes_present\n","  \n","  # Compute the mean across the batch axis. Shape: Scalar\n","  mean_iou = tf.math.reduce_mean(iou)\n","  \n","  return mean_iou"]},{"cell_type":"markdown","metadata":{"id":"35527322"},"source":["## 5. Model [10 Points]\n","\n","<font style=\"color:red\">In this section you have to define your segmentation model.</font>"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"5ba50553"},"outputs":[],"source":["def convolution_block(block_input, num_filters=256, kernel_size=3, dilation_rate=1, padding=\"same\", use_bias=False):\n","  x = tfk.layers.Conv2D(num_filters, kernel_size=kernel_size, dilation_rate=dilation_rate, padding=\"same\", use_bias=use_bias)(block_input)\n","  x = tfk.layers.BatchNormalization()(x)\n","  return tfk.layers.Activation('relu')(x)\n","\n","\n","def dilate_spatial_pyramid_pooling(dspp_input):\n","  dims = dspp_input.shape\n","  # Create a 1x1 feature map using AveragePooling2D.\n","  x = tfk.layers.AveragePooling2D(pool_size=(dims[1], dims[2]))(dspp_input)\n","  x = convolution_block(x, kernel_size=1, use_bias=True)\n","\n","  # Upsample the feature map to the original size.\n","  out_pool = tfk.layers.UpSampling2D(size=(dims[1], dims[2]), interpolation=\"bilinear\")(x)\n","\n","  # Create feature maps of the same shape with different dilation rates.\n","  out_1  = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n","  out_6  = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n","  out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n","  out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n","\n","  # Combine all the feature maps and process them through a 1x1 convolutional block.\n","  x = tfk.layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n","  output = convolution_block(x, kernel_size=1)\n","\n","  return output\n","\n","\n","def deeplabv3plus(num_classes, shape):\n","  model_input = tfk.layers.Input(shape=shape)\n","  preprocessing = tfk.applications.resnet50.preprocess_input(model_input)\n","  backbone = tfk.applications.resnet50.ResNet50(weights=\"imagenet\", include_top=False, input_tensor=preprocessing)\n","\n","  # Set all layers in the backbone as trainable.\n","  for layer in backbone.layers:\n","    layer.trainable = True\n","\n","  # Obtain a (lower resolution) feature map from the backbone.\n","  # Shape: (14, 14, 256)\n","  input_a = backbone.get_layer(\"conv4_block6_2_relu\").output\n","\n","  # Pass through Atrous Spatial Pyramid Pooling to obtain features at various scales.\n","  input_a = dilate_spatial_pyramid_pooling(input_a)\n","\n","  # Upsample the concatenated features.\n","  input_a = tfk.layers.UpSampling2D(size=(4, 4), interpolation=\"bilinear\")(input_a)\n","\n","  # Obtain a second (higher resolution) feature map from the backbone and apply convolution.\n","  # Shape: (56, 56, 64)\n","  input_b = backbone.get_layer(\"conv2_block3_2_relu\").output\n","  input_b = convolution_block(input_b, num_filters=256, kernel_size=1)\n","\n","  # Concatenate both sets of feature maps and perform final decoder processing.\n","  x = tfk.layers.Concatenate(axis=-1)([input_a, input_b])\n","  x = convolution_block(x)\n","  x = convolution_block(x)\n","  x = tfk.layers.UpSampling2D(size=(4, 4), interpolation=\"bilinear\")(x)\n","\n","  # Apply 1x1 convolution to limit the depth of the feature maps to the number of classes.\n","  outputs = tfk.layers.Conv2D(num_classes, kernel_size=(1, 1), padding=\"same\")(x)\n","\n","  model_output = tfk.layers.Activation('softmax')(outputs)\n","  model = tfk.Model(inputs=model_input, outputs=model_output)\n","\n","  return model\n","\n","\n","def cb_checkpoint(save_best_only:bool=True, save_weights_only:bool=False):\n","  \"\"\"Call back for saving the best model during training.\n","  \"\"\"\n","  # Create a new checkpoint directory every time.\n","  if not os.path.exists(TrainingConfig.CHECKPOINT_DIR):\n","    os.makedirs(TrainingConfig.CHECKPOINT_DIR)\n","\n","  return tfk.callbacks.ModelCheckpoint(\n","    filepath=TrainingConfig.CHECKPOINT_DIR,\n","    monitor=\"accuracy\",\n","    mode=\"max\",\n","    # monitor='val_loss',\n","    # mode='auto',\n","    save_best_only=save_best_only,\n","    save_weights_only=save_weights_only,\n","    verbose=1,\n","  )"]},{"cell_type":"markdown","metadata":{"id":"7b8b72d6"},"source":["## 6. Train and Plot Results [7 Points]\n","\n","<font style=\"color:red\">In this section you have to:</font>\n","    \n","1. Implement the training pipeline.\n","\n","2. Train the model.\n","\n","3. Plot loss and metrics for training and validation set.\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["model = deeplabv3plus(num_classes=DatasetConfig.NUM_CLASSES, shape=DatasetConfig.SHAPE)\n","\n","\n","# PiecewiseConstantDecay\n","boundaries = [1300, 2600]\n","# values = [0.001, 0.0001, 0.00001]\n","values = [0.01, 0.001, 0.0001]\n","lrs_piecewise_constant_decay = tfk.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)\n","\n","\n","# \"InverseTimeDecay\"\n","lrs_inverse_time_decay = tfk.optimizers.schedules.InverseTimeDecay(\n","  0.1, \n","  decay_steps=1, \n","  decay_rate=0.5,\n",")\n","\n","\n","# ExponentialDecay\n","# Define the ExponentialDecay LR Scheduler.                                                            \n","learning_rate_fn = tfk.optimizers.schedules.ExponentialDecay(\n","  0.001,\n","  decay_steps=100, \n","  decay_rate=0.96,\n","  staircase=True,\n",")\n","\n","\n","model.compile(\n","  tf.keras.optimizers.Adam(learning_rate=lrs_piecewise_constant_decay, momentum=0.9),\n","  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n","  metrics=['accuracy', mean_iou],\n",")\n","\n","\n","# model.compile(\n","#   optimizer=tfk.optimizers.Adam(learning_rate=TrainingConfig.LEARNING_RATE),   \n","#   loss=tfk.losses.SparseCategoricalCrossentropy(from_logits=False),\n","#   metrics=['accuracy', mean_iou],\n","# )"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"d4287e79"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n"]},{"name":"stderr","output_type":"stream","text":["2023-12-22 14:35:32.459158: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 536870912 exceeds 10% of free system memory.\n","2023-12-22 14:35:32.773105: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 536870912 exceeds 10% of free system memory.\n"]},{"name":"stdout","output_type":"stream","text":["  1/184 [..............................] - ETA: 53:15 - loss: 2.5794 - accuracy: 0.1287 - mean_iou: 0.0225"]},{"name":"stderr","output_type":"stream","text":["2023-12-22 14:35:36.940538: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 536870912 exceeds 10% of free system memory.\n","2023-12-22 14:35:37.260478: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 536870912 exceeds 10% of free system memory.\n"]},{"name":"stdout","output_type":"stream","text":["  2/184 [..............................] - ETA: 12:58 - loss: 3.4374 - accuracy: 0.1153 - mean_iou: 0.0239"]},{"name":"stderr","output_type":"stream","text":["2023-12-22 14:35:41.181185: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 536870912 exceeds 10% of free system memory.\n"]},{"name":"stdout","output_type":"stream","text":["121/184 [==================>...........] - ETA: 4:00 - loss: 1.4540 - accuracy: 0.5421 - mean_iou: 0.2070"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTrainingConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m  \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m  \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m  \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m  \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmodel_checkpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n","File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n","File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n","File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n","File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n","File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n","File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["history = model.fit(\n","  train_ds,\n","  validation_data=valid_ds,\n","  epochs=TrainingConfig.EPOCHS, \n","  use_multiprocessing=TrainingConfig.MULTIPROCESSING,\n","  callbacks=[\n","    cb_checkpoint(),\n","  ],\n","  workers=4,\n","  verbose=1,\n",")"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"1b4b9932"},"outputs":[{"ename":"NameError","evalue":"name 'history' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m   plt\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Pixel accuracy.\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     28\u001b[0m valid_acc \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Mean IoU.\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"]}],"source":["def plot_results(metrics, ylabel=None, ylim=None, metric_name=None, color=None):\n","  fig, ax = plt.subplots(figsize=(18, 5))\n","\n","  if not (isinstance(metric_name, list) or isinstance(metric_name, tuple)):\n","    metrics = [metrics,]\n","    metric_name = [metric_name,]\n","      \n","  for idx, metric in enumerate(metrics):    \n","    ax.plot(metric, color=color[idx])\n","  \n","  plt.xlabel(\"Epoch\")\n","  plt.ylabel(ylabel)\n","  plt.title(ylabel)\n","  plt.xlim([0, TrainingConfig.EPOCHS-1])\n","  plt.ylim(ylim)\n","  # Tailor x-axis tick marks\n","  ax.xaxis.set_major_locator(MultipleLocator(5))\n","  ax.xaxis.set_major_formatter(FormatStrFormatter('%d'))\n","  ax.xaxis.set_minor_locator(MultipleLocator(1))\n","  plt.grid(True)\n","  plt.legend(metric_name)   \n","  plt.show(block=block_plot)\n","  plt.close()\n","\n","\n","# Pixel accuracy.\n","train_acc = history.history[\"accuracy\"]\n","valid_acc = history.history[\"val_accuracy\"]\n","\n","# Mean IoU.\n","train_iou = history.history[\"mean_iou\"]\n","valid_iou = history.history[\"val_mean_iou\"]\n","\n","plot_results(\n","  [train_acc, valid_acc],\n","  ylabel=\"Pixel Accuracy\",\n","  ylim = [0.5, 1.0],\n","  metric_name=[\"Training Accuracy\", \"Validation Accuracy\"],\n","  color=[\"g\", \"b\"]\n",")\n","\n","plot_results(\n","  [train_iou, valid_iou ], \n","  ylabel=\"Mean IoU\",\n","  ylim = [0.0, 1.0],\n","  metric_name=[\"Training IoU\", \"Validation IoU\"],\n","  color=[\"g\", \"b\"]\n",")\n","\n","# Loss\n","train_loss = history.history[\"loss\"]\n","valid_loss = history.history[\"val_loss\"]\n","max_loss = max(max(train_loss), max(valid_loss))\n","\n","plot_results(\n","  [train_loss, valid_loss],        \n","  ylabel=\"Cross Entropy Loss\", \n","  ylim=[0.0, max_loss],\n","  metric_name=[\"Training Loss\", \"Validation Loss\"],\n","  color=[\"g\", \"b\"]\n",")"]},{"cell_type":"markdown","metadata":{"id":"a78806d6"},"source":["## 7. Inference [3 Points]\n","\n","\n","<font style=\"color:red\">Plot inferences on 3 random images from your validation set.</font>\n","\n","<font style=\"color:green\">Note: Inference should be performed on the original image size and not on any resized image.</font>\n","\n","**E.g., if the original image has (H, W) of (512, 384) and your model returns a final output (H', W') shape of (256, 256).\n","You need to resize your outputs to size (512, 384) before plotting.**\n","\n","\n","Example plots:\n","\n","---\n","---\n","\n","<img src='https://learnopencv.com/wp-content/uploads/2022/06/c4-project3-Preds.png'>\n","\n","\n","---\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trained_model = tf.keras.models.load_model(TrainingConfig.CHECKPOINT_DIR, custom_objects={'mean_iou':mean_iou})"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["evaluate = trained_model.evaluate(valid_ds)\n","\n","print(f\"Model evaluation accuracy: {evaluate[1]*100:.2f} %\")\n","print(f\"Model evaluation mean IoU: {evaluate[2]*100:.2f} %\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def inference(model, dataset):\n","\n","  num_batches_to_process = InferenceConfig.NUM_BATCHES\n","\n","  for idx, data in enumerate(dataset):\n","    batch_img, batch_mask = data[0], data[1]\n","    pred_all = (model.predict(batch_img)).astype('float32')\n","    pred_all = pred_all.argmax(-1)\n","    batch_img = (batch_img).astype('uint8')\n","    if idx == num_batches_to_process:\n","      break\n","\n","    for i in range(0, len(batch_img)):\n","      fig = plt.figure(figsize=(20,8))\n","      # Display the original image.\n","      ax1 = fig.add_subplot(1,4,1)\n","      ax1.imshow(batch_img[i])\n","      ax1.title.set_text('Actual frame')\n","      plt.axis('off')\n","      \n","      # Display the ground truth mask.\n","      true_mask = batch_mask[i]\n","      ax2 = fig.add_subplot(1,4,2)\n","      ax2.set_title('Ground truth labels')\n","      ax2.imshow(true_mask)\n","      plt.axis('off')\n","      \n","      # Display the predicted segmentation mask. \n","      pred_mask = pred_all[i]\n","      ax3 = fig.add_subplot(1,4,3)\n","      ax3.set_title('Predicted labels')\n","      ax3.imshow(pred_mask)\n","      plt.axis('off')\n","\n","      # Display the predicted segmentation mask overlayed on the original image.\n","      pred_mask_rgb = num_to_rgb(pred_all[i], color_map=DatasetConfig.ID2COLOR)\n","      overlayed_image = image_overlay((batch_img[i]), np.array(pred_mask_rgb.astype('uint8')))\n","      ax4 = fig.add_subplot(1,4,4)\n","      ax4.set_title('Overlayed image')\n","      ax4.imshow(overlayed_image)\n","      plt.axis('off')\n","\n","      plt.show()\n","\n","inference(trained_model, valid_ds)"]},{"cell_type":"markdown","metadata":{"id":"dff77375"},"source":["## 8. Prepare Submission CSV [10 Points]\n","\n","\n","<font style=\"color:red\">Write your code to prepare the submission CSV file.</font>\n","\n","**Note:** In the submission file, you have to write pixel values in ***Run-length Encoded format***. This is done to reduce the size of submission file.\n","\n","**A note on how Run-length encoding for Kaggle competitions** (taken from [HuBMAP - Hacking the Kidney Competition](https://www.kaggle.com/code/leahscherschel/run-length-encoding/notebook))\n","\n","> *To reduce the submission file size, teams must submit segmentation results using run-length encoding on the pixel values. Instead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length. E.g. `0 3` implies starting at `pixel 0` and running a total of `3` pixels `(0,1,2)`. The competition format requires a space-delimited list of pairs. For example, `0 3 10 5` implies pixels `0,1,2`, and `10,11,12,13,14` are to be included in the mask. The metric checks that the pairs are sorted and positive and the decoded pixel values are not duplicated. The pixels are numbered from **top to bottom, then left to right:** 0 is pixel (0,0), 1 is pixel (1,0), and 2 is pixel (2,0), etc.*\n"]},{"cell_type":"markdown","metadata":{"id":"_I4bXCVKubmz"},"source":["[Here another blog post to understand what is Encoded Pixels.](https://medium.com/analytics-vidhya/generating-masks-from-encoded-pixels-semantic-segmentation-18635e834ad0)"]},{"cell_type":"markdown","metadata":{"id":"3219f363"},"source":["**You are free to use any code available online as long as the output format is maintained.**  "]},{"cell_type":"markdown","metadata":{"id":"470e5bfb"},"source":["----\n","\n","1. For generating a `submission.csv` file, you need to encode the model outputs for images in the testing set using **Run-Length Encoding (RLE)**.\n","\n","2. RLE is performed on binary masks. So RLE is performed on each output channel. \n","\n","3. RLE needs to performed on predictions outputs having the same size as the original image\n","\n","4. Similar to **Inference section**, if the original image has (H, W) of (512, 384) and your model returns a final output (H', W') shape of (256, 256). You need to resize your outputs to size (512, 384) before performing RLE. \n","\n","\n","**Required Columns:** \n","1. **`IMG_ID`**\n","2. **`EncodedString`**\n","\n","----"]},{"cell_type":"markdown","metadata":{"id":"f45ddf2e"},"source":["As RLE works on one binary mask at a time and we have multiple classes per image. Each image should have `10`  rows in your `submission.csv` file.\n","\n","Example: For Image name = `test_image.jpg`\n","\n","\n","**IMG_IDs** will be \n","\n","```python\n","test_image_00\n","test_image_01\n","test_image_02\n","test_image_03\n","test_image_04\n","test_image_05\n","test_image_06\n","test_image_07\n","test_image_08\n","test_image_09\n","```\n","\n","**Format:** `{image_name}_{class_id}` (no extention)\n","\n","----\n","\n","**`sample_submission.csv`.**\n","\n","```html\n","IMG_ID,EncodedString\n","10181_00,\n","10181_01,\n","10181_02,\n","10181_03,\n","10181_04,\n","10181_05,227 2 707 2 1187 2 1667 2\n","10181_06,28317 4 28797 4 29277 4 29757 4 35035 6 35515 6 35995 6 36475 6 36955 6 37435 6 37917 4 38397 4\n","10181_07,\n","10181_08,\n","10181_09,\n","10806_00,\n","10806_01,\n","10806_02,140621 4 141101 4\n","10806_03,\n","10806_04,3143 12 3159 6 3623 12 3639 6 4025 14 409\n","```\n","\n","\n","**`EncodedString`** can be blank if there's no prediction available.\n","\n","----"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b45da764"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d95996ce"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8c43c003"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"1147fadd"},"source":["## 9. Kaggle Submission Score [50 Points]\n","\n","You need a minimum dice score of `80%` on the **Public test leaderboard**.<br>\n","\n","If the Dice score is less than `80%`, you gain no points for this section.\n","\n","**Dice scores will be rounded-off to the nearest integer.**\n","\n","Submit `submission.csv` (prediction for images in test.csv) in the **Submit Predictions** tab in Kaggle to get evaluated for this section.\n","\n","Please share your profile link, user id and score achieved.\n","\n","```\n","URL:\n","Profile Name:\n","Points Scored:\n","```"]},{"cell_type":"markdown","metadata":{"id":"4f981ae6"},"source":["**Upon completing the project, <font style=\"color:red\">upload the notebook to the lab for grading and feedback.</font>**"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":4}
